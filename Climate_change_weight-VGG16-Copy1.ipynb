{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.utils import class_weight\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout, Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from shutil import copyfile\n",
    "\n",
    "# Copyright 2014-2017 Bert Carremans\n",
    "# Author: Bert Carremans <bertcarremans.be>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "def img_train_test_split(img_source_dir, train_size):   \n",
    "    if not (isinstance(img_source_dir, str)):\n",
    "        raise AttributeError('img_source_dir must be a string')\n",
    "        \n",
    "    if not (isinstance(train_size, float)):\n",
    "        raise AttributeError('train_size must be a float')\n",
    "        \n",
    "    # Set up empty folder structure if not exists\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    else:\n",
    "        if not os.path.exists('data/train'):\n",
    "            os.makedirs('data/train')\n",
    "        if not os.path.exists('data/validation'):\n",
    "            os.makedirs('data/validation')\n",
    "    \n",
    "    # Get the subdirectories in the main image folder\n",
    "    \n",
    "    subdirs = [subdir for subdir in os.listdir(img_source_dir) if os.path.isdir(os.path.join(img_source_dir, subdir))]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_fullpath = os.path.join(img_source_dir, subdir)\n",
    "        if len(os.listdir(subdir_fullpath)) == 0:\n",
    "            print(subdir_fullpath + ' is empty')\n",
    "            break\n",
    "\n",
    "        train_subdir = os.path.join('data/train', subdir)\n",
    "        validation_subdir = os.path.join('data/validation', subdir)\n",
    "\n",
    "        # Create subdirectories in train and validation folders\n",
    "        if not os.path.exists(train_subdir):\n",
    "            os.makedirs(train_subdir)\n",
    "\n",
    "        if not os.path.exists(validation_subdir):\n",
    "            os.makedirs(validation_subdir)\n",
    "\n",
    "        train_counter = 0\n",
    "        validation_counter = 0\n",
    "\n",
    "        # Randomly assign an image to train or validation folder\n",
    "        for filename in os.listdir(subdir_fullpath):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n",
    "                fileparts = filename.split('.')\n",
    "\n",
    "                if random.uniform(0, 1) <= train_size:\n",
    "                    copyfile(os.path.join(subdir_fullpath, filename), os.path.join(train_subdir, str(train_counter) + '.' + fileparts[1]))\n",
    "                    train_counter += 1\n",
    "                else:\n",
    "                    copyfile(os.path.join(subdir_fullpath, filename), os.path.join(validation_subdir, str(validation_counter) + '.' + fileparts[1]))\n",
    "                    validation_counter += 1\n",
    "                    \n",
    "        print('Copied ' + str(train_counter) + ' images to data/train/' + subdir)\n",
    "        print('Copied ' + str(validation_counter) + ' images to data/validation/' + subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 91 images to data/train/korean_rice\n",
      "Copied 40 images to data/validation/korean_rice\n",
      "Copied 26 images to data/train/disposable_diaper\n",
      "Copied 9 images to data/validation/disposable_diaper\n",
      "Copied 164 images to data/train/coffee\n",
      "Copied 69 images to data/validation/coffee\n",
      "Copied 46 images to data/train/china_wheat\n",
      "Copied 17 images to data/validation/china_wheat\n",
      "Copied 45 images to data/train/quiver_tree\n",
      "Copied 29 images to data/validation/quiver_tree\n",
      "Copied 190 images to data/train/aluminum_can\n",
      "Copied 98 images to data/validation/aluminum_can\n",
      "Copied 63 images to data/train/styrofoam\n",
      "Copied 22 images to data/validation/styrofoam\n",
      "Copied 99 images to data/train/newt\n",
      "Copied 43 images to data/validation/newt\n",
      "Copied 73 images to data/train/carpenter_bee\n",
      "Copied 33 images to data/validation/carpenter_bee\n",
      "Copied 47 images to data/train/long_distance_migrant_bird\n",
      "Copied 24 images to data/validation/long_distance_migrant_bird\n",
      "Copied 81 images to data/train/plastic_bag\n",
      "Copied 29 images to data/validation/plastic_bag\n",
      "Copied 37 images to data/train/ink_cartridge\n",
      "Copied 13 images to data/validation/ink_cartridge\n",
      "Copied 182 images to data/train/barley\n",
      "Copied 92 images to data/validation/barley\n",
      "Copied 14 images to data/train/spring_peeper\n",
      "Copied 10 images to data/validation/spring_peeper\n",
      "Copied 203 images to data/train/light_bulb\n",
      "Copied 82 images to data/validation/light_bulb\n",
      "Copied 58 images to data/train/mason_bee\n",
      "Copied 22 images to data/validation/mason_bee\n",
      "Copied 185 images to data/train/tiger\n",
      "Copied 71 images to data/validation/tiger\n",
      "Copied 204 images to data/train/frog\n",
      "Copied 81 images to data/validation/frog\n",
      "Copied 143 images to data/train/adélie_penguin\n",
      "Copied 58 images to data/validation/adélie_penguin\n",
      "Copied 31 images to data/train/electronic_waste\n",
      "Copied 9 images to data/validation/electronic_waste\n",
      "Copied 147 images to data/train/tin_can\n",
      "Copied 53 images to data/validation/tin_can\n",
      "Copied 185 images to data/train/polar_bear\n",
      "Copied 82 images to data/validation/polar_bear\n",
      "Copied 122 images to data/train/coral\n",
      "Copied 63 images to data/validation/coral\n",
      "Copied 163 images to data/train/plastic_bottle\n",
      "Copied 67 images to data/validation/plastic_bottle\n",
      "Copied 102 images to data/train/gecko\n",
      "Copied 28 images to data/validation/gecko\n",
      "Copied 143 images to data/train/fishing_line\n",
      "Copied 52 images to data/validation/fishing_line\n",
      "Copied 193 images to data/train/crocodilian\n",
      "Copied 72 images to data/validation/crocodilian\n",
      "Copied 163 images to data/train/lizard\n",
      "Copied 71 images to data/validation/lizard\n",
      "Copied 197 images to data/train/glass_bottle\n",
      "Copied 101 images to data/validation/glass_bottle\n",
      "Copied 115 images to data/train/bumblebee\n",
      "Copied 39 images to data/validation/bumblebee\n",
      "Copied 0 images to data/train/.ipynb_checkpoints\n",
      "Copied 0 images to data/validation/.ipynb_checkpoints\n",
      "Copied 161 images to data/train/banana\n",
      "Copied 68 images to data/validation/banana\n",
      "Copied 34 images to data/train/phytoplankton\n",
      "Copied 14 images to data/validation/phytoplankton\n",
      "Copied 179 images to data/train/snake\n",
      "Copied 88 images to data/validation/snake\n",
      "Copied 112 images to data/train/forest_bird\n",
      "Copied 43 images to data/validation/forest_bird\n",
      "Copied 89 images to data/train/shark\n",
      "Copied 55 images to data/validation/shark\n",
      "Copied 127 images to data/train/lion\n",
      "Copied 51 images to data/validation/lion\n",
      "Copied 149 images to data/train/corn\n",
      "Copied 70 images to data/validation/corn\n",
      "Copied 78 images to data/train/snowy_plover\n",
      "Copied 39 images to data/validation/snowy_plover\n",
      "Copied 150 images to data/train/toad\n",
      "Copied 68 images to data/validation/toad\n",
      "Copied 161 images to data/train/cacao\n",
      "Copied 84 images to data/validation/cacao\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/home/sogang03/school/ClimateChange_tsne/data/128px_image_square_final\"\n",
    "images = glob.glob(os.path.join(img_path,\"*/*.jpg\" ))\n",
    "\n",
    "\n",
    "img_train_test_split(img_path, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['korean_rice', 'disposable_diaper', 'coffee', 'china_wheat', 'quiver_tree', 'aluminum_can', 'styrofoam', 'newt', 'carpenter_bee', 'long_distance_migrant_bird', 'plastic_bag', 'ink_cartridge', 'barley', 'spring_peeper', 'light_bulb', 'mason_bee', 'tiger', 'frog', 'adélie_penguin', 'electronic_waste', 'tin_can', 'polar_bear', 'coral', 'plastic_bottle', 'gecko', 'fishing_line', 'crocodilian', 'lizard', 'glass_bottle', 'bumblebee', 'banana', 'phytoplankton', 'snake', 'forest_bird', 'shark', 'lion', 'corn', 'snowy_plover', 'toad', 'cacao']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "a = glob.glob('/home/sogang03/school/ClimateChange_tsne/data/train/*')\n",
    "class_names = []\n",
    "for i in range(len(a)):\n",
    "    p = a[i][52:]\n",
    "    class_names.append(p)\n",
    "    \n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5162 images belonging to 40 classes.\n",
      "Found 2433 images belonging to 40 classes.\n"
     ]
    }
   ],
   "source": [
    "#preprocess image \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_data_dir = './data/train'\n",
    "validation_data_dir = './data/validation'\n",
    "input_shape=(224,224,3)\n",
    "\n",
    "trdata = ImageDataGenerator( \n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=False,\n",
    "        fill_mode='reflect')\n",
    "traindata = trdata.flow_from_directory(directory= train_data_dir,classes=class_names, color_mode='rgb', class_mode='categorical', target_size=(224,224), batch_size=16)\n",
    "vldata = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=False,\n",
    "        fill_mode='reflect')\n",
    "valdata = vldata.flow_from_directory(directory= validation_data_dir,classes=class_names,  color_mode='rgb', class_mode='categorical', target_size=(224,224), batch_size=16)\n",
    "\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced', np.unique(traindata.classes), traindata.classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "img = load_img(os.path.join(train_data_dir, 'lizard/2.jpg'))# PIL 이미지\n",
    "\n",
    "x = img_to_array(img)  # (3, 150, 150) 크기의 NumPy 배열\n",
    "x = x.reshape((1,) + x.shape)  # (1, 3, 150, 150) 크기의 NumPy 배열\n",
    "\n",
    "# 아래 .flow() 함수는 임의 변환된 이미지를 배치 단위로 생성해서\n",
    "# 지정된 `preview/` 폴더에 저장합니다.\n",
    "i = 0\n",
    "for batch in trdata.flow(x, batch_size=1,\n",
    "                          save_to_dir='./preview', save_prefix='lizard', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # 이미지 20장을 생성하고 마칩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f4b819ca090> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b819ca050> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b0c0dd9d0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f4b819c5b50> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b83bdba90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b83bda290> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f4b81826bd0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b8182dbd0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b81838ad0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b8183f210> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f4b81843450> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b81859610> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b817e5510> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b817eb950> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f4b817f8b90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b81804d10> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b8180af50> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4b81818310> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f4b817a4a10> True\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers except the last 4 layers\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    " \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                41000     \n",
      "=================================================================\n",
      "Total params: 40,446,824\n",
      "Trainable params: 32,811,560\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    " \n",
    "# Add new layers\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(40, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata 323\n",
      "valdata 153\n"
     ]
    }
   ],
   "source": [
    "print(\"traindata\", len(traindata))\n",
    "print(\"valdata\", len(valdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                41000     \n",
      "=================================================================\n",
      "Total params: 40,446,824\n",
      "Trainable params: 32,811,560\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "opt = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 25s 127ms/step - loss: 3.1820 - accuracy: 0.1650 - val_loss: 2.8697 - val_accuracy: 0.2812\n",
      "Epoch 2/100\n",
      "  2/200 [..............................] - ETA: 16s - loss: 2.9536 - accuracy: 0.2188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sogang03/.local/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/home/sogang03/.local/lib/python3.7/site-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 25s 126ms/step - loss: 2.6083 - accuracy: 0.2959 - val_loss: 2.8070 - val_accuracy: 0.3187\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 2.2837 - accuracy: 0.3688 - val_loss: 2.3155 - val_accuracy: 0.3625\n",
      "Epoch 4/100\n",
      " 62/200 [========>.....................] - ETA: 17s - loss: 2.1725 - accuracy: 0.4067"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1e1f38bd1c48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mearly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvaldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"/tmp/weights.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "hist = model.fit_generator(steps_per_epoch=200,generator=traindata, validation_data= valdata, validation_steps=10,epochs=100, callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('climateChange_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
