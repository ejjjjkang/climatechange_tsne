{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.utils import class_weight\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout, Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from shutil import copyfile\n",
    "\n",
    "# Copyright 2014-2017 Bert Carremans\n",
    "# Author: Bert Carremans <bertcarremans.be>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "def img_train_test_split(img_source_dir, train_size):   \n",
    "    if not (isinstance(img_source_dir, str)):\n",
    "        raise AttributeError('img_source_dir must be a string')\n",
    "        \n",
    "    if not (isinstance(train_size, float)):\n",
    "        raise AttributeError('train_size must be a float')\n",
    "        \n",
    "    # Set up empty folder structure if not exists\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    else:\n",
    "        if not os.path.exists('data/train'):\n",
    "            os.makedirs('data/train')\n",
    "        if not os.path.exists('data/validation'):\n",
    "            os.makedirs('data/validation')\n",
    "    \n",
    "    # Get the subdirectories in the main image folder\n",
    "    \n",
    "    subdirs = [subdir for subdir in os.listdir(img_source_dir) if os.path.isdir(os.path.join(img_source_dir, subdir))]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_fullpath = os.path.join(img_source_dir, subdir)\n",
    "        if len(os.listdir(subdir_fullpath)) == 0:\n",
    "            print(subdir_fullpath + ' is empty')\n",
    "            break\n",
    "\n",
    "        train_subdir = os.path.join('data/train', subdir)\n",
    "        validation_subdir = os.path.join('data/validation', subdir)\n",
    "\n",
    "        # Create subdirectories in train and validation folders\n",
    "        if not os.path.exists(train_subdir):\n",
    "            os.makedirs(train_subdir)\n",
    "\n",
    "        if not os.path.exists(validation_subdir):\n",
    "            os.makedirs(validation_subdir)\n",
    "\n",
    "        train_counter = 0\n",
    "        validation_counter = 0\n",
    "\n",
    "        # Randomly assign an image to train or validation folder\n",
    "        for filename in os.listdir(subdir_fullpath):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n",
    "                fileparts = filename.split('.')\n",
    "\n",
    "                if random.uniform(0, 1) <= train_size:\n",
    "                    copyfile(os.path.join(subdir_fullpath, filename), os.path.join(train_subdir, str(train_counter) + '.' + fileparts[1]))\n",
    "                    train_counter += 1\n",
    "                else:\n",
    "                    copyfile(os.path.join(subdir_fullpath, filename), os.path.join(validation_subdir, str(validation_counter) + '.' + fileparts[1]))\n",
    "                    validation_counter += 1\n",
    "                    \n",
    "        print('Copied ' + str(train_counter) + ' images to data/train/' + subdir)\n",
    "        print('Copied ' + str(validation_counter) + ' images to data/validation/' + subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 88 images to data/train/korean_rice\n",
      "Copied 43 images to data/validation/korean_rice\n",
      "Copied 26 images to data/train/disposable_diaper\n",
      "Copied 9 images to data/validation/disposable_diaper\n",
      "Copied 160 images to data/train/coffee\n",
      "Copied 73 images to data/validation/coffee\n",
      "Copied 37 images to data/train/china_wheat\n",
      "Copied 26 images to data/validation/china_wheat\n",
      "Copied 50 images to data/train/quiver_tree\n",
      "Copied 24 images to data/validation/quiver_tree\n",
      "Copied 193 images to data/train/aluminum_can\n",
      "Copied 95 images to data/validation/aluminum_can\n",
      "Copied 60 images to data/train/styrofoam\n",
      "Copied 25 images to data/validation/styrofoam\n",
      "Copied 109 images to data/train/newt\n",
      "Copied 33 images to data/validation/newt\n",
      "Copied 71 images to data/train/carpenter_bee\n",
      "Copied 35 images to data/validation/carpenter_bee\n",
      "Copied 52 images to data/train/long_distance_migrant_bird\n",
      "Copied 19 images to data/validation/long_distance_migrant_bird\n",
      "Copied 82 images to data/train/plastic_bag\n",
      "Copied 28 images to data/validation/plastic_bag\n",
      "Copied 37 images to data/train/ink_cartridge\n",
      "Copied 13 images to data/validation/ink_cartridge\n",
      "Copied 195 images to data/train/barley\n",
      "Copied 79 images to data/validation/barley\n",
      "Copied 20 images to data/train/spring_peeper\n",
      "Copied 4 images to data/validation/spring_peeper\n",
      "Copied 202 images to data/train/light_bulb\n",
      "Copied 83 images to data/validation/light_bulb\n",
      "Copied 57 images to data/train/mason_bee\n",
      "Copied 23 images to data/validation/mason_bee\n",
      "Copied 175 images to data/train/tiger\n",
      "Copied 81 images to data/validation/tiger\n",
      "Copied 204 images to data/train/frog\n",
      "Copied 81 images to data/validation/frog\n",
      "Copied 152 images to data/train/adélie_penguin\n",
      "Copied 49 images to data/validation/adélie_penguin\n",
      "Copied 28 images to data/train/electronic_waste\n",
      "Copied 12 images to data/validation/electronic_waste\n",
      "Copied 143 images to data/train/tin_can\n",
      "Copied 57 images to data/validation/tin_can\n",
      "Copied 189 images to data/train/polar_bear\n",
      "Copied 78 images to data/validation/polar_bear\n",
      "Copied 122 images to data/train/coral\n",
      "Copied 63 images to data/validation/coral\n",
      "Copied 156 images to data/train/plastic_bottle\n",
      "Copied 74 images to data/validation/plastic_bottle\n",
      "Copied 98 images to data/train/gecko\n",
      "Copied 32 images to data/validation/gecko\n",
      "Copied 141 images to data/train/fishing_line\n",
      "Copied 54 images to data/validation/fishing_line\n",
      "Copied 182 images to data/train/crocodilian\n",
      "Copied 83 images to data/validation/crocodilian\n",
      "Copied 163 images to data/train/lizard\n",
      "Copied 71 images to data/validation/lizard\n",
      "Copied 225 images to data/train/glass_bottle\n",
      "Copied 73 images to data/validation/glass_bottle\n",
      "Copied 121 images to data/train/bumblebee\n",
      "Copied 33 images to data/validation/bumblebee\n",
      "Copied 0 images to data/train/.ipynb_checkpoints\n",
      "Copied 0 images to data/validation/.ipynb_checkpoints\n",
      "Copied 168 images to data/train/banana\n",
      "Copied 61 images to data/validation/banana\n",
      "Copied 28 images to data/train/phytoplankton\n",
      "Copied 20 images to data/validation/phytoplankton\n",
      "Copied 187 images to data/train/snake\n",
      "Copied 80 images to data/validation/snake\n",
      "Copied 111 images to data/train/forest_bird\n",
      "Copied 44 images to data/validation/forest_bird\n",
      "Copied 89 images to data/train/shark\n",
      "Copied 55 images to data/validation/shark\n",
      "Copied 125 images to data/train/lion\n",
      "Copied 53 images to data/validation/lion\n",
      "Copied 142 images to data/train/corn\n",
      "Copied 77 images to data/validation/corn\n",
      "Copied 80 images to data/train/snowy_plover\n",
      "Copied 37 images to data/validation/snowy_plover\n",
      "Copied 168 images to data/train/toad\n",
      "Copied 50 images to data/validation/toad\n",
      "Copied 165 images to data/train/cacao\n",
      "Copied 80 images to data/validation/cacao\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/home/sogang03/school/ClimateChange_tsne/data/128px_image_square_final\"\n",
    "images = glob.glob(os.path.join(img_path,\"*/*.jpg\" ))\n",
    "\n",
    "\n",
    "img_train_test_split(img_path, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['korean_rice', 'disposable_diaper', 'coffee', 'china_wheat', 'quiver_tree', 'aluminum_can', 'styrofoam', 'newt', 'carpenter_bee', 'long_distance_migrant_bird', 'plastic_bag', 'ink_cartridge', 'barley', 'spring_peeper', 'light_bulb', 'mason_bee', 'tiger', 'frog', 'adélie_penguin', 'electronic_waste', 'tin_can', 'polar_bear', 'coral', 'plastic_bottle', 'gecko', 'fishing_line', 'crocodilian', 'lizard', 'glass_bottle', 'bumblebee', 'banana', 'phytoplankton', 'snake', 'forest_bird', 'shark', 'lion', 'corn', 'snowy_plover', 'toad', 'cacao']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "a = glob.glob('/home/sogang03/school/ClimateChange_tsne/data/train/*')\n",
    "class_names = []\n",
    "for i in range(len(a)):\n",
    "    p = a[i][52:]\n",
    "    class_names.append(p)\n",
    "    \n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5146 images belonging to 40 classes.\n",
      "Found 2420 images belonging to 40 classes.\n"
     ]
    }
   ],
   "source": [
    "#preprocess image \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_data_dir = './data/train'\n",
    "validation_data_dir = './data/validation'\n",
    "input_shape=(224,224,3)\n",
    "\n",
    "trdata = ImageDataGenerator( \n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='reflect')\n",
    "traindata = trdata.flow_from_directory(directory= train_data_dir,classes=class_names, color_mode='rgb', class_mode='categorical', target_size=(224,224), batch_size=16)\n",
    "vldata = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='reflect')\n",
    "valdata = vldata.flow_from_directory(directory= validation_data_dir,classes=class_names,  color_mode='rgb', class_mode='categorical', target_size=(224,224), batch_size=16)\n",
    "\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced', np.unique(traindata.classes), traindata.classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f94b6f6d510> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f94b6f63bd0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f94b6f63610> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f94b6f6df50> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f94cc050ed0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f955e375090> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f95d2137110> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f95d213a5d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f95d2141250> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f95d214d9d0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f95d215a910> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f95d2161dd0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f95d216df10> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f95d20fb110> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f95d2106310> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f95d210b810> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f95d2118710> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f95d211eb50> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f95d212c6d0> True\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers except the last 4 layers\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    " \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                41000     \n",
      "=================================================================\n",
      "Total params: 40,446,824\n",
      "Trainable params: 32,811,560\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    " \n",
    "# Add new layers\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(40, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata 322\n",
      "valdata 152\n"
     ]
    }
   ],
   "source": [
    "print(\"traindata\", len(traindata))\n",
    "print(\"valdata\", len(valdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                41000     \n",
      "=================================================================\n",
      "Total params: 40,446,824\n",
      "Trainable params: 32,811,560\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "opt = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 3.2046 - accuracy: 0.1594 - val_loss: 2.7121 - val_accuracy: 0.2875\n",
      "Epoch 2/100\n",
      "  2/200 [..............................] - ETA: 16s - loss: 2.9539 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sogang03/.local/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/home/sogang03/.local/lib/python3.7/site-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 27s 133ms/step - loss: 2.5695 - accuracy: 0.3025 - val_loss: 2.5709 - val_accuracy: 0.3688\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 27s 134ms/step - loss: 2.2300 - accuracy: 0.3854 - val_loss: 1.9344 - val_accuracy: 0.3812\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 27s 134ms/step - loss: 2.0036 - accuracy: 0.4374 - val_loss: 1.7677 - val_accuracy: 0.4500\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 1.9028 - accuracy: 0.4772 - val_loss: 2.2973 - val_accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 1.7650 - accuracy: 0.4915 - val_loss: 1.8156 - val_accuracy: 0.4875\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 1.6476 - accuracy: 0.5332 - val_loss: 0.9191 - val_accuracy: 0.4500\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 1.6123 - accuracy: 0.5472 - val_loss: 1.5601 - val_accuracy: 0.4875\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.5554 - accuracy: 0.5535 - val_loss: 2.4272 - val_accuracy: 0.5813\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.4970 - accuracy: 0.5631 - val_loss: 1.0499 - val_accuracy: 0.5250\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 1.4255 - accuracy: 0.5883 - val_loss: 1.3510 - val_accuracy: 0.5375\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 27s 134ms/step - loss: 1.3768 - accuracy: 0.6074 - val_loss: 1.9599 - val_accuracy: 0.5250\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.3716 - accuracy: 0.6031 - val_loss: 1.7098 - val_accuracy: 0.5625\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 27s 136ms/step - loss: 1.3597 - accuracy: 0.6099 - val_loss: 1.5370 - val_accuracy: 0.5813\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 27s 136ms/step - loss: 1.2878 - accuracy: 0.6216 - val_loss: 2.6999 - val_accuracy: 0.5437\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 1.3309 - accuracy: 0.6225 - val_loss: 2.7878 - val_accuracy: 0.4730\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 27s 133ms/step - loss: 1.2597 - accuracy: 0.6421 - val_loss: 1.2371 - val_accuracy: 0.5625\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 27s 136ms/step - loss: 1.2039 - accuracy: 0.6637 - val_loss: 0.9671 - val_accuracy: 0.5625\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 27s 136ms/step - loss: 1.1992 - accuracy: 0.6562 - val_loss: 1.8087 - val_accuracy: 0.5500\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 1.2343 - accuracy: 0.6503 - val_loss: 1.7494 - val_accuracy: 0.5750\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.1974 - accuracy: 0.6553 - val_loss: 1.3975 - val_accuracy: 0.5813\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 1.1835 - accuracy: 0.6753 - val_loss: 5.1114 - val_accuracy: 0.5188\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 1.2283 - accuracy: 0.6587 - val_loss: 4.0001 - val_accuracy: 0.5375\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.2241 - accuracy: 0.6606 - val_loss: 0.7668 - val_accuracy: 0.6000\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 27s 136ms/step - loss: 1.1679 - accuracy: 0.6819 - val_loss: 1.2209 - val_accuracy: 0.5562\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 27s 136ms/step - loss: 1.2812 - accuracy: 0.6628 - val_loss: 2.0466 - val_accuracy: 0.6187\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 1.2718 - accuracy: 0.6650 - val_loss: 1.2687 - val_accuracy: 0.5625\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 1.1906 - accuracy: 0.6822 - val_loss: 1.1319 - val_accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.3265 - accuracy: 0.6569 - val_loss: 3.4115 - val_accuracy: 0.5375\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.2520 - accuracy: 0.6716 - val_loss: 3.4043 - val_accuracy: 0.4437\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 28s 142ms/step - loss: 1.2984 - accuracy: 0.6681 - val_loss: 2.5242 - val_accuracy: 0.5270\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 27s 133ms/step - loss: 1.2275 - accuracy: 0.6703 - val_loss: 2.7292 - val_accuracy: 0.6062\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 1.2162 - accuracy: 0.6728 - val_loss: 1.8110 - val_accuracy: 0.5938\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 1.3358 - accuracy: 0.6606 - val_loss: 1.8613 - val_accuracy: 0.6125\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.2979 - accuracy: 0.6603 - val_loss: 2.8615 - val_accuracy: 0.5750\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 27s 134ms/step - loss: 1.2819 - accuracy: 0.6725 - val_loss: 2.6444 - val_accuracy: 0.5813\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.3874 - accuracy: 0.6444 - val_loss: 1.5615 - val_accuracy: 0.5000\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 25s 127ms/step - loss: 1.2555 - accuracy: 0.6672 - val_loss: 1.3175 - val_accuracy: 0.5750\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.2859 - accuracy: 0.6634 - val_loss: 1.7615 - val_accuracy: 0.5125\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.3093 - accuracy: 0.6590 - val_loss: 1.2282 - val_accuracy: 0.6062\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 1.3387 - accuracy: 0.6541 - val_loss: 2.0661 - val_accuracy: 0.5188\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 28s 142ms/step - loss: 1.4333 - accuracy: 0.6456 - val_loss: 4.2298 - val_accuracy: 0.5375\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 26s 130ms/step - loss: 1.4092 - accuracy: 0.6493 - val_loss: 2.0479 - val_accuracy: 0.5375\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 25s 126ms/step - loss: 1.4473 - accuracy: 0.6431 - val_loss: 4.2090 - val_accuracy: 0.5500\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.4556 - accuracy: 0.6478 - val_loss: 1.8625 - val_accuracy: 0.5312\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 25s 126ms/step - loss: 1.4855 - accuracy: 0.6421 - val_loss: 2.3920 - val_accuracy: 0.5135\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.5666 - accuracy: 0.6319 - val_loss: 1.9783 - val_accuracy: 0.5625\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.5235 - accuracy: 0.6337 - val_loss: 4.2715 - val_accuracy: 0.5125\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.4934 - accuracy: 0.6341 - val_loss: 1.5689 - val_accuracy: 0.5688\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 25s 126ms/step - loss: 1.5022 - accuracy: 0.6315 - val_loss: 2.1269 - val_accuracy: 0.5000\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 1.5360 - accuracy: 0.6331 - val_loss: 2.9078 - val_accuracy: 0.4625\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.5649 - accuracy: 0.6209 - val_loss: 4.8597 - val_accuracy: 0.5188\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.5374 - accuracy: 0.6309 - val_loss: 1.5777 - val_accuracy: 0.5562\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 25s 126ms/step - loss: 1.3941 - accuracy: 0.6522 - val_loss: 2.7948 - val_accuracy: 0.5312\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.6375 - accuracy: 0.6259 - val_loss: 1.4980 - val_accuracy: 0.4875\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 1.5175 - accuracy: 0.6331 - val_loss: 1.9016 - val_accuracy: 0.5562\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 1.5025 - accuracy: 0.6378 - val_loss: 0.7523 - val_accuracy: 0.6438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.6684 - accuracy: 0.6115 - val_loss: 1.6885 - val_accuracy: 0.5375\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 1.5830 - accuracy: 0.6219 - val_loss: 6.3346 - val_accuracy: 0.4875\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.6795 - accuracy: 0.5983 - val_loss: 6.8914 - val_accuracy: 0.5188\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 24s 122ms/step - loss: 1.5515 - accuracy: 0.6240 - val_loss: 1.7484 - val_accuracy: 0.6419\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 24s 122ms/step - loss: 1.5668 - accuracy: 0.6168 - val_loss: 1.4440 - val_accuracy: 0.5437\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.5978 - accuracy: 0.6118 - val_loss: 8.3712 - val_accuracy: 0.5188\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.7489 - accuracy: 0.5994 - val_loss: 0.8690 - val_accuracy: 0.4437\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 1.7575 - accuracy: 0.6050 - val_loss: 2.3643 - val_accuracy: 0.4437\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.7143 - accuracy: 0.6115 - val_loss: 3.8113 - val_accuracy: 0.5000\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 1.7751 - accuracy: 0.6108 - val_loss: 2.1089 - val_accuracy: 0.5375\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.9009 - accuracy: 0.5884 - val_loss: 1.1639 - val_accuracy: 0.5125\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.6885 - accuracy: 0.6119 - val_loss: 4.1555 - val_accuracy: 0.5437\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.7277 - accuracy: 0.6408 - val_loss: 2.2310 - val_accuracy: 0.5312\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.9743 - accuracy: 0.5816 - val_loss: 1.2071 - val_accuracy: 0.4375\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 24s 122ms/step - loss: 1.6712 - accuracy: 0.6115 - val_loss: 12.2244 - val_accuracy: 0.4750\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 1.9207 - accuracy: 0.5722 - val_loss: 2.2758 - val_accuracy: 0.5625\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 2.0576 - accuracy: 0.5676 - val_loss: 5.0688 - val_accuracy: 0.4437\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 29s 145ms/step - loss: 1.9013 - accuracy: 0.5831 - val_loss: 5.5333 - val_accuracy: 0.5500\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 28s 141ms/step - loss: 1.7235 - accuracy: 0.5917 - val_loss: 4.2134 - val_accuracy: 0.5135\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 26s 132ms/step - loss: 1.9522 - accuracy: 0.5861 - val_loss: 1.3181 - val_accuracy: 0.5375\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.8101 - accuracy: 0.5933 - val_loss: 1.5721 - val_accuracy: 0.5437\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.8874 - accuracy: 0.5800 - val_loss: 3.6172 - val_accuracy: 0.4000\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.9587 - accuracy: 0.5657 - val_loss: 1.2507 - val_accuracy: 0.4250\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.8665 - accuracy: 0.5791 - val_loss: 3.4805 - val_accuracy: 0.5188\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 1.9659 - accuracy: 0.5764 - val_loss: 2.6858 - val_accuracy: 0.4812\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.9604 - accuracy: 0.5634 - val_loss: 1.6539 - val_accuracy: 0.5312\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 2.2478 - accuracy: 0.5382 - val_loss: 4.5739 - val_accuracy: 0.4875\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 1.9638 - accuracy: 0.5553 - val_loss: 1.2906 - val_accuracy: 0.4750\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 2.0948 - accuracy: 0.5429 - val_loss: 3.2977 - val_accuracy: 0.4375\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 2.2554 - accuracy: 0.5510 - val_loss: 1.7055 - val_accuracy: 0.5188\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 2.1801 - accuracy: 0.5545 - val_loss: 3.9565 - val_accuracy: 0.4750\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 2.1154 - accuracy: 0.5300 - val_loss: 3.1507 - val_accuracy: 0.4812\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 2.1864 - accuracy: 0.5269 - val_loss: 1.9196 - val_accuracy: 0.3875\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 2.4209 - accuracy: 0.5213 - val_loss: 5.0345 - val_accuracy: 0.4375\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 25s 127ms/step - loss: 2.2865 - accuracy: 0.5184 - val_loss: 2.7789 - val_accuracy: 0.5135\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 2.3368 - accuracy: 0.5141 - val_loss: 13.4309 - val_accuracy: 0.4750\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 2.2801 - accuracy: 0.5075 - val_loss: 0.8260 - val_accuracy: 0.4500\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 25s 126ms/step - loss: 2.6391 - accuracy: 0.4881 - val_loss: 1.1215 - val_accuracy: 0.5312\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 2.3067 - accuracy: 0.5019 - val_loss: 2.1722 - val_accuracy: 0.4500\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 2.3801 - accuracy: 0.5088 - val_loss: 5.4199 - val_accuracy: 0.5375\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 2.3683 - accuracy: 0.5272 - val_loss: 2.4620 - val_accuracy: 0.3438\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 2.5950 - accuracy: 0.4837 - val_loss: 4.1666 - val_accuracy: 0.4563\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 2.3680 - accuracy: 0.5000 - val_loss: 2.6016 - val_accuracy: 0.4688\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"/tmp/weights.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "hist = model.fit_generator(steps_per_epoch=200,generator=traindata, validation_data= valdata, validation_steps=10,epochs=100, callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('climateChange_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
